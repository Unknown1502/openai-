{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Competition Winner - Google Colab Edition\n",
    "\n",
    "This notebook provides an interactive interface for running the enhanced gpt-oss-20b vulnerability competition in Google Colab.\n",
    "\n",
    "## Features\n",
    "- 60+ sophisticated attack scenarios\n",
    "- Multiple testing strategies (standard, aggressive, subtle)\n",
    "- Four scanning modes (comprehensive, targeted, random, quick)\n",
    "- Automatic memory management for Colab GPUs\n",
    "- Detailed result analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's clone the repository and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (if not already done)\n",
    "import os\n",
    "if not os.path.exists('/content/openai-'):\n",
    "    !git clone https://github.com/Unknown1502/openai-.git /content/openai-\n",
    "    %cd /content/openai-\n",
    "else:\n",
    "    %cd /content/openai-\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Colab-specific setup\n",
    "!python COLAB_COMPETITION_WINNER.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Monitoring\n",
    "\n",
    "Let's set up memory monitoring to track GPU usage during the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitor and display system resources.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        gpu_percent = (gpu_memory / gpu_total) * 100\n",
    "        \n",
    "        print(f\"üñ•Ô∏è GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üìä GPU Memory: {gpu_memory:.2f}/{gpu_total:.2f} GB ({gpu_percent:.1f}%)\")\n",
    "    \n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    ram_percent = psutil.virtual_memory().percent\n",
    "    \n",
    "    print(f\"üíª CPU Usage: {cpu_percent}%\")\n",
    "    print(f\"üß† RAM Usage: {ram_percent}%\")\n",
    "\n",
    "# Display current resources\n",
    "monitor_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Test Run\n",
    "\n",
    "Let's run a quick test to ensure everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced attack vectors\n",
    "from src.competition.enhanced_attack_vectors import (\n",
    "    ENHANCED_COMPETITION_ATTACK_VECTORS,\n",
    "    get_categories,\n",
    "    get_attack_count\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(ENHANCED_COMPETITION_ATTACK_VECTORS)} attack scenarios\")\n",
    "print(f\"\\nüìä Attack Distribution by Category:\")\n",
    "\n",
    "attack_counts = get_attack_count()\n",
    "for category, count in sorted(attack_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {category}: {count} attacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attack distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "categories = list(attack_counts.keys())\n",
    "counts = list(attack_counts.values())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(categories, counts, color='skyblue', edgecolor='navy')\n",
    "plt.xlabel('Attack Category')\n",
    "plt.ylabel('Number of Scenarios')\n",
    "plt.title('Distribution of Attack Scenarios by Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Competition - Quick Mode\n",
    "\n",
    "Let's start with a quick scan to test the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick competition scan\n",
    "!python run_enhanced_competition.py --mode quick --strategy standard --output-dir quick_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display quick results\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    with open('quick_results/enhanced_competition_report_standard_quick.json', 'r') as f:\n",
    "        quick_report = json.load(f)\n",
    "    \n",
    "    print(\"üèÜ Quick Scan Results:\")\n",
    "    print(f\"Scenarios Tested: {quick_report['summary']['total_scenarios_tested']}\")\n",
    "    print(f\"Vulnerabilities Found: {quick_report['summary']['vulnerabilities_found']}\")\n",
    "    print(f\"Success Rate: {quick_report['summary']['success_rate']}\")\n",
    "    \n",
    "    if quick_report['findings']:\n",
    "        print(\"\\nüîç Sample Findings:\")\n",
    "        for i, finding in enumerate(quick_report['findings'][:3]):\n",
    "            print(f\"\\n{i+1}. {finding['issue_summary']['issue_title']}\")\n",
    "            print(f\"   Category: {finding['issue_summary']['topic_area']}\")\n",
    "            print(f\"   Severity: {finding['metadata']['scenario_severity']}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No results file found. Make sure the scan completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Full Competition Scans\n",
    "\n",
    "Now let's run different competition strategies. Choose based on your available time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy selector\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "strategy_widget = widgets.Dropdown(\n",
    "    options=['standard', 'aggressive', 'subtle'],\n",
    "    value='standard',\n",
    "    description='Strategy:',\n",
    ")\n",
    "\n",
    "mode_widget = widgets.Dropdown(\n",
    "    options=['quick', 'random', 'targeted', 'comprehensive'],\n",
    "    value='targeted',\n",
    "    description='Mode:',\n",
    ")\n",
    "\n",
    "def run_competition(strategy, mode):\n",
    "    print(f\"Running competition with strategy='{strategy}' and mode='{mode}'...\")\n",
    "    output_dir = f\"{strategy}_{mode}_results\"\n",
    "    !python run_enhanced_competition.py --strategy {strategy} --mode {mode} --output-dir {output_dir}\n",
    "\n",
    "print(\"Select your competition parameters:\")\n",
    "display(strategy_widget)\n",
    "display(mode_widget)\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Competition\")\n",
    "run_button.on_click(lambda b: run_competition(strategy_widget.value, mode_widget.value))\n",
    "display(run_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results\n",
    "\n",
    "Let's analyze the competition results with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load all report files\n",
    "report_files = glob.glob('*_results/enhanced_competition_report_*.json')\n",
    "all_reports = []\n",
    "\n",
    "for file in report_files:\n",
    "    with open(file, 'r') as f:\n",
    "        report = json.load(f)\n",
    "        report['file'] = file\n",
    "        all_reports.append(report)\n",
    "\n",
    "if all_reports:\n",
    "    # Create comparison dataframe\n",
    "    comparison_data = []\n",
    "    for report in all_reports:\n",
    "        comparison_data.append({\n",
    "            'Strategy': report['summary']['strategy'],\n",
    "            'Mode': report['summary']['mode'],\n",
    "            'Scenarios': report['summary']['total_scenarios_tested'],\n",
    "            'Vulnerabilities': report['summary']['vulnerabilities_found'],\n",
    "            'Success Rate': float(report['summary']['success_rate'].rstrip('%'))\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Display results table\n",
    "    print(\"üìä Competition Results Summary:\")\n",
    "    display(df)\n",
    "    \n",
    "    # Visualize success rates\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df, x='Strategy', y='Success Rate', hue='Mode')\n",
    "    plt.title('Success Rate by Strategy and Mode')\n",
    "    plt.ylabel('Success Rate (%)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No report files found. Run some competitions first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vulnerability categories across all reports\n",
    "if all_reports:\n",
    "    category_vulnerabilities = {}\n",
    "    \n",
    "    for report in all_reports:\n",
    "        for finding in report['findings']:\n",
    "            category = finding['issue_summary']['topic_area']\n",
    "            if category not in category_vulnerabilities:\n",
    "                category_vulnerabilities[category] = 0\n",
    "            category_vulnerabilities[category] += 1\n",
    "    \n",
    "    # Create pie chart\n",
    "    if category_vulnerabilities:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.pie(category_vulnerabilities.values(), \n",
    "                labels=category_vulnerabilities.keys(), \n",
    "                autopct='%1.1f%%',\n",
    "                startangle=90)\n",
    "        plt.title('Vulnerability Distribution by Category')\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "        \n",
    "        # Top recommendations\n",
    "        print(\"\\nüéØ Top Recommendations:\")\n",
    "        latest_report = all_reports[-1]\n",
    "        for i, rec in enumerate(latest_report['recommendations'][:5], 1):\n",
    "            print(f\"{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results\n",
    "\n",
    "Export your findings for submission or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all findings\n",
    "all_findings = []\n",
    "for report in all_reports:\n",
    "    all_findings.extend(report['findings'])\n",
    "\n",
    "# Remove duplicates based on issue title\n",
    "unique_findings = {}\n",
    "for finding in all_findings:\n",
    "    title = finding['issue_summary']['issue_title']\n",
    "    if title not in unique_findings:\n",
    "        unique_findings[title] = finding\n",
    "\n",
    "# Save consolidated findings\n",
    "consolidated_report = {\n",
    "    'total_unique_findings': len(unique_findings),\n",
    "    'findings': list(unique_findings.values()),\n",
    "    'metadata': {\n",
    "        'reports_analyzed': len(all_reports),\n",
    "        'total_scenarios_tested': sum(r['summary']['total_scenarios_tested'] for r in all_reports),\n",
    "        'timestamp': datetime.utcnow().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('consolidated_competition_results.json', 'w') as f:\n",
    "    json.dump(consolidated_report, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Exported {len(unique_findings)} unique findings to consolidated_competition_results.json\")\n",
    "\n",
    "# Create submission-ready format\n",
    "submission_findings = []\n",
    "for finding in list(unique_findings.values())[:10]:  # Top 10 findings\n",
    "    submission_findings.append({\n",
    "        'title': finding['issue_summary']['issue_title'],\n",
    "        'category': finding['issue_summary']['topic_area'],\n",
    "        'severity': finding['metadata']['scenario_severity'],\n",
    "        'description': finding['issue_summary']['overview']\n",
    "    })\n",
    "\n",
    "with open('submission_findings.json', 'w') as f:\n",
    "    json.dump({'findings': submission_findings}, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Created submission_findings.json with top findings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Download your results:\")\n",
    "files.download('consolidated_competition_results.json')\n",
    "files.download('submission_findings.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "Clean up GPU memory and temporary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ GPU memory cleared\")\n",
    "\n",
    "# Final resource check\n",
    "monitor_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully run the enhanced competition scanner. Your findings are ready for submission.\n",
    "\n",
    "### Next Steps:\n",
    "1. Review the consolidated findings\n",
    "2. Select the most impactful vulnerabilities for submission\n",
    "3. Add any manual findings you discovered\n",
    "4. Submit your results to the competition\n",
    "\n",
    "Good luck! üèÜ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
