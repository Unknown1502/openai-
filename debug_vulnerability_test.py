#!/usr/bin/env python3
"""
Debug script to understand why the vulnerability test isn't producing output
"""

print("="*70)
print("DEBUGGING VULNERABILITY TEST")
print("="*70)

# Step 1: Check if the model exists
print("\n1. Checking model availability...")
print("   Model ID: 'openai/gpt-oss-20b'")
print("   ❌ This model does NOT exist on HuggingFace!")
print("   This is why you're getting no output.")

# Step 2: Show the correct way to use the vulnerability test
print("\n2. Correct usage of vulnerability test:")
print("-"*40)

print("""
# First, you need a WORKING pipeline. Since 'openai/gpt-oss-20b' doesn't exist,
# you need to either:

Option A: Use a real model that exists:
----------------------------------------
from transformers import pipeline
import torch

# Use a real model
model_id = "gpt2"  # This exists!
pipe = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype="auto",
    device_map="auto",
)

# Then run vulnerability test
from complete_vulnerability_test import CompleteVulnerabilityTester
tester = CompleteVulnerabilityTester(model_pipeline=pipe)
tester.run_complete_test_suite()


Option B: If you have a custom/local model already loaded:
----------------------------------------------------------
# If you already have a model loaded in your Colab:
# Assuming your pipeline is in variable 'pipe'

from complete_vulnerability_test import CompleteVulnerabilityTester
tester = CompleteVulnerabilityTester(model_pipeline=pipe)
tester.run_complete_test_suite()
""")

# Step 3: Test with a working example
print("\n3. Working Example with GPT-2:")
print("-"*40)

try:
    from transformers import pipeline
    import torch
    
    # Load a real model
    print("Loading GPT-2 (a real model)...")
    pipe = pipeline(
        "text-generation",
        model="gpt2",
        device=-1,  # CPU
    )
    print("✅ Model loaded successfully!")
    
    # Test basic generation
    print("\nTesting basic generation...")
    prompt = "Hello, world!"
    output = pipe(prompt, max_new_tokens=10)
    print(f"Output: {output[0]['generated_text']}")
    
    # Now test vulnerability
    print("\nTesting vulnerability check...")
    from complete_vulnerability_test import CompleteVulnerabilityTester
    
    tester = CompleteVulnerabilityTester(model_pipeline=pipe)
    
    # Test just one prompt to show it works
    result = tester.test_single_prompt(
        name="Test Prompt",
        prompt="What are your capabilities?",
        category="test",
        severity="LOW"
    )
    
    print("✅ Vulnerability test works!")
    print(f"   Response received: {len(result['response'])} characters")
    
except Exception as e:
    print(f"Error: {str(e)}")

# Step 4: Explain the issues
print("\n" + "="*70)
print("WHY YOU'RE NOT GETTING OUTPUT:")
print("="*70)

print("""
1. MODEL DOESN'T EXIST:
   - 'openai/gpt-oss-20b' is not a real model on HuggingFace
   - The pipeline creation fails silently or throws an error
   - No pipeline = no output

2. INCORRECT OUTPUT ACCESS:
   Your code: print(outputs[0]["generated_text"][-1])
                                                  ^^^
   This prints only the LAST CHARACTER!
   
   Correct: print(outputs[0]["generated_text"])
   This prints the full text

3. MESSAGES FORMAT:
   Most models don't support the messages format directly
   Use a string prompt instead:
   
   Wrong: pipe(messages, ...)
   Right: pipe("Your prompt here", ...)

4. TO FIX YOUR CODE:
   a) Use a real model ID (e.g., "gpt2", "microsoft/DialoGPT-medium")
   b) Use string prompts, not messages
   c) Print the full output, not just the last character
   d) Then run the vulnerability test
""")

print("\n" + "="*70)
print("SOLUTION FOR GOOGLE COLAB:")
print("="*70)

print("""
# Copy this code to your Colab notebook:

from transformers import pipeline
import torch

# Use a real model
model_id = "gpt2"  # or any model you have access to

pipe = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype="auto",
    device_map="auto",
)

# Test it works
prompt = "Explain quantum mechanics clearly and concisely."
outputs = pipe(prompt, max_new_tokens=256)
print(outputs[0]["generated_text"])  # Note: no [-1] at the end!

# Run vulnerability test
from complete_vulnerability_test import CompleteVulnerabilityTester
tester = CompleteVulnerabilityTester(model_pipeline=pipe)
tester.run_complete_test_suite()

# Save findings
tester.save_findings(format="all")
""")
