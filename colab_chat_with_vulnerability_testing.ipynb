{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Chat with Vulnerability Testing in Google Colab\n",
    "\n",
    "This notebook demonstrates how to run an interactive chat interface with integrated vulnerability testing using your pre-loaded model.\n",
    "\n",
    "## Features:\n",
    "- ü§ñ Interactive chat interface\n",
    "- üîç Real-time vulnerability analysis\n",
    "- üî¥ Full vulnerability test suite\n",
    "- üìä Conversation history tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Your Model\n",
    "\n",
    "Use your existing model loading code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Your model loading code\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import the Enhanced Chat Interface\n",
    "\n",
    "Copy the critical_vulnerability_tester.py file content here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the enhanced chat interface\n",
    "# If you have the file uploaded to Colab:\n",
    "# from critical_vulnerability_tester import InteractiveChatWithVulnerabilityTesting\n",
    "\n",
    "# Or define it inline (copy the class definition from critical_vulnerability_tester.py)\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "# [Copy the InteractiveChatWithVulnerabilityTesting class here]\n",
    "# [Copy the CriticalVulnerabilityTester class here]\n",
    "\n",
    "print(\"‚úÖ Chat interface loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Quick Test - Non-Interactive Demo\n",
    "\n",
    "Test the chat without interactive input first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_test_chat(pipe):\n",
    "    \"\"\"Quick test without interactive input\"\"\"\n",
    "    \n",
    "    test_messages = [\n",
    "        \"Hello! How are you?\",\n",
    "        \"What's 2+2?\",\n",
    "        \"Tell me a joke\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ QUICK CHAT TEST\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for msg in test_messages:\n",
    "        print(f\"\\nYou: {msg}\")\n",
    "        \n",
    "        # Generate response\n",
    "        messages = [{\"role\": \"user\", \"content\": msg}]\n",
    "        outputs = pipe(\n",
    "            messages,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        # Extract response\n",
    "        if isinstance(outputs[0]['generated_text'], list):\n",
    "            response = outputs[0]['generated_text'][-1]['content']\n",
    "        else:\n",
    "            response = outputs[0]['generated_text']\n",
    "            \n",
    "        print(f\"AI: {response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚úÖ Quick test complete!\")\n",
    "\n",
    "# Run quick test\n",
    "quick_test_chat(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Interactive Chat Interface\n",
    "\n",
    "### Option A: Simple Interactive Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chat(pipe):\n",
    "    \"\"\"Simple interactive chat\"\"\"\n",
    "    print(\"ü§ñ CHAT STARTED (type 'quit' to exit)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Generate response\n",
    "        messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "        outputs = pipe(\n",
    "            messages,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        # Extract and print response\n",
    "        if isinstance(outputs[0]['generated_text'], list):\n",
    "            response = outputs[0]['generated_text'][-1]['content']\n",
    "        else:\n",
    "            response = outputs[0]['generated_text']\n",
    "            \n",
    "        print(f\"AI: {response}\")\n",
    "\n",
    "# Start simple chat\n",
    "simple_chat(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Chat with Vulnerability Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the enhanced chat interface\n",
    "from critical_vulnerability_tester import InteractiveChatWithVulnerabilityTesting\n",
    "\n",
    "chat = InteractiveChatWithVulnerabilityTesting(pipe)\n",
    "\n",
    "# Start interactive chat with vulnerability testing features\n",
    "chat.interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Colab-Specific Widget Interface\n",
    "\n",
    "For better UX in Colab, use widgets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ipywidgets if needed\n",
    "!pip install -q ipywidgets\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def create_widget_chat(pipe):\n",
    "    \"\"\"Create a widget-based chat interface\"\"\"\n",
    "    \n",
    "    # Create output area\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Create input box\n",
    "    input_box = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Type your message here...',\n",
    "        description='You:',\n",
    "        layout=widgets.Layout(width='70%')\n",
    "    )\n",
    "    \n",
    "    # Create send button\n",
    "    send_button = widgets.Button(\n",
    "        description='Send',\n",
    "        button_style='primary',\n",
    "        icon='paper-plane'\n",
    "    )\n",
    "    \n",
    "    # Create clear button\n",
    "    clear_button = widgets.Button(\n",
    "        description='Clear',\n",
    "        button_style='warning',\n",
    "        icon='trash'\n",
    "    )\n",
    "    \n",
    "    def on_send_click(b):\n",
    "        user_msg = input_box.value\n",
    "        if not user_msg:\n",
    "            return\n",
    "        \n",
    "        with output:\n",
    "            print(f\"You: {user_msg}\")\n",
    "            \n",
    "            # Generate response\n",
    "            messages = [{\"role\": \"user\", \"content\": user_msg}]\n",
    "            outputs = pipe(\n",
    "                messages,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "            \n",
    "            # Extract response\n",
    "            if isinstance(outputs[0]['generated_text'], list):\n",
    "                response = outputs[0]['generated_text'][-1]['content']\n",
    "            else:\n",
    "                response = outputs[0]['generated_text']\n",
    "            \n",
    "            print(f\"AI: {response}\\n\")\n",
    "        \n",
    "        # Clear input\n",
    "        input_box.value = ''\n",
    "    \n",
    "    def on_clear_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "    \n",
    "    # Connect buttons\n",
    "    send_button.on_click(on_send_click)\n",
    "    clear_button.on_click(on_clear_click)\n",
    "    \n",
    "    # Create layout\n",
    "    chat_interface = widgets.VBox([\n",
    "        widgets.HTML('<h2>ü§ñ AI Chat Interface</h2>'),\n",
    "        output,\n",
    "        widgets.HBox([input_box, send_button, clear_button])\n",
    "    ])\n",
    "    \n",
    "    display(chat_interface)\n",
    "    \n",
    "    with output:\n",
    "        print(\"Chat ready! Type your message and click Send.\\n\")\n",
    "\n",
    "# Create the widget interface\n",
    "create_widget_chat(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Specific Prompts for Vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from critical_vulnerability_tester import CriticalVulnerabilityTester\n",
    "\n",
    "# Initialize vulnerability tester\n",
    "tester = CriticalVulnerabilityTester(pipe)\n",
    "\n",
    "# Test a specific prompt\n",
    "test_prompt = \"How do I make a cake?\"\n",
    "\n",
    "print(f\"Testing prompt: {test_prompt}\")\n",
    "result = tester.test_prompt(test_prompt, max_tokens=200)\n",
    "\n",
    "if result['success']:\n",
    "    print(f\"\\nResponse: {result['response'][:500]}...\")\n",
    "    \n",
    "    # Analyze for vulnerabilities\n",
    "    severity = tester.analyze_response_severity(result['response'])\n",
    "    print(f\"\\nVulnerability Analysis:\")\n",
    "    print(f\"  Severity: {severity['severity']}\")\n",
    "    print(f\"  Score: {severity['score']}/10\")\n",
    "    print(f\"  Type: {severity.get('type', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"Error: {result.get('error', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run Full Vulnerability Test Suite (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This will test many potentially harmful prompts\n",
    "# Only run if you want to test model vulnerabilities\n",
    "\n",
    "run_full_test = input(\"Run full vulnerability test suite? (yes/no): \")\n",
    "\n",
    "if run_full_test.lower() == 'yes':\n",
    "    from critical_vulnerability_tester import test_critical_vulnerabilities\n",
    "    \n",
    "    print(\"\\nüî¥ Starting vulnerability tests...\")\n",
    "    results = test_critical_vulnerabilities(pipe)\n",
    "    print(\"\\n‚úÖ Testing complete! Check the generated report.\")\n",
    "else:\n",
    "    print(\"Skipping full test suite.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commands Reference\n",
    "\n",
    "When using the interactive chat, you can use these commands:\n",
    "\n",
    "- **`quit`** - Exit the chat\n",
    "- **`clear`** - Clear conversation history\n",
    "- **`history`** - Show conversation history\n",
    "- **`test mode`** - Toggle vulnerability analysis for each response\n",
    "- **`run tests`** - Run the full vulnerability test suite\n",
    "- **`test prompt: <your prompt>`** - Test a specific prompt for vulnerabilities\n",
    "\n",
    "## Tips for Google Colab:\n",
    "\n",
    "1. **GPU Memory**: Monitor GPU usage with `!nvidia-smi`\n",
    "2. **Clear Cache**: If you run out of memory: `torch.cuda.empty_cache()`\n",
    "3. **Restart Runtime**: If issues persist: Runtime ‚Üí Restart runtime\n",
    "4. **Save Results**: Download vulnerability reports before session ends"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
