#!/usr/bin/env python3
"""
Example script showing how to use the complete vulnerability test with your loaded model
"""

from transformers import pipeline
import torch

# Your model loading code
print("Loading model...")
model_id = "openai/gpt-oss-20b"  # Note: This model doesn't exist on HuggingFace

try:
    # Try to load the pipeline
    pipe = pipeline(
        "text-generation",
        model=model_id,
        torch_dtype="auto",
        device_map="auto",
    )
    print("Model loaded successfully!")
    
    # Test the model with a simple prompt first
    print("\nTesting basic generation...")
    messages = [
        {"role": "user", "content": "Explain quantum mechanics clearly and concisely."},
    ]
    
    # Note: Most models expect a string, not messages format
    # Convert messages to a simple prompt
    prompt = messages[0]["content"]
    
    outputs = pipe(
        prompt,  # Use string prompt instead of messages
        max_new_tokens=256,
    )
    
    # Print the output correctly
    print("Model output:", outputs[0]["generated_text"])
    
    # Now run vulnerability testing
    print("\n" + "="*70)
    print("RUNNING VULNERABILITY TESTS")
    print("="*70)
    
    # Import the vulnerability testing module
    from complete_vulnerability_test import CompleteVulnerabilityTester, quick_vulnerability_check
    
    # Option 1: Quick test (recommended to start)
    print("\nRunning quick vulnerability check...")
    results = quick_vulnerability_check(pipe=pipe)
    
    # Option 2: Full test suite (uncomment to use)
    # print("\nRunning complete vulnerability test suite...")
    # tester = CompleteVulnerabilityTester(model_pipeline=pipe)
    # tester.run_complete_test_suite()
    # 
    # # Save findings in all formats
    # print("\nSaving findings...")
    # findings_dir = tester.save_findings(format="all")
    # print(f"Findings saved to: {findings_dir}")
    
    print("\n✅ Vulnerability testing completed!")
    print(f"Total tests run: {len(results)}")
    
    # Print summary of results
    vulnerable_count = sum(1 for r in results if r.get('vulnerable', False))
    refused_count = sum(1 for r in results if r.get('refused', False))
    
    print(f"Vulnerable responses: {vulnerable_count}")
    print(f"Refused responses: {refused_count}")
    
except Exception as e:
    print(f"\n❌ Error: {str(e)}")
    print("\nNote: The model 'openai/gpt-oss-20b' doesn't exist on HuggingFace.")
    print("This is likely why you're not getting output.")
    print("\nTo test with a real model, try one of these:")
    print("  - model_id = 'gpt2'")
    print("  - model_id = 'microsoft/DialoGPT-medium'")
    print("  - model_id = 'EleutherAI/gpt-neo-125M'")
    
    # Show example with a working model
    print("\n" + "="*70)
    print("EXAMPLE WITH WORKING MODEL (GPT-2)")
    print("="*70)
    
    try:
        # Load a real model that exists
        print("Loading GPT-2...")
        pipe = pipeline(
            "text-generation",
            model="gpt2",
            torch_dtype="auto" if torch.cuda.is_available() else torch.float32,
            device=0 if torch.cuda.is_available() else -1,
        )
        print("GPT-2 loaded successfully!")
        
        # Test generation
        prompt = "Explain quantum mechanics clearly and concisely."
        outputs = pipe(
            prompt,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
        )
        print(f"\nGPT-2 output: {outputs[0]['generated_text']}")
        
        # Run vulnerability test on GPT-2
        print("\nRunning vulnerability test on GPT-2...")
        from complete_vulnerability_test import quick_vulnerability_check
        
        results = quick_vulnerability_check(pipe=pipe)
        print(f"✅ Tested GPT-2 with {len(results)} vulnerability prompts")
        
    except Exception as e2:
        print(f"Error with GPT-2: {str(e2)}")

print("\n" + "="*70)
print("USAGE INSTRUCTIONS")
print("="*70)
print("""
To use the vulnerability testing in your code:

1. Import the module:
   from complete_vulnerability_test import CompleteVulnerabilityTester

2. Create a tester with your pipeline:
   tester = CompleteVulnerabilityTester(model_pipeline=pipe)

3. Run tests:
   tester.run_complete_test_suite()

4. Save findings:
   tester.save_findings(format="all")

The issue with no output is likely because:
1. The model "openai/gpt-oss-20b" doesn't exist on HuggingFace
2. The pipeline expects a string prompt, not messages format
3. The output access was incorrect (should be outputs[0]["generated_text"])
""")
