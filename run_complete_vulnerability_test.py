#!/usr/bin/env python3
"""
Complete vulnerability test with direct execution
Run this file directly to test your model
"""

from transformers import pipeline
import torch
import sys
import os

# Add current directory to path to import the test module
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def main():
    """
    Main function to run vulnerability testing with model loading
    """
    print("="*70)
    print("COMPLETE VULNERABILITY TESTING SUITE")
    print("="*70)
    
    # Model loading code
    print("\nLoading model...")
    model_id = "openai/gpt-oss-20b"  # Note: This model doesn't exist on HuggingFace
    
    try:
        # Try to load the pipeline
        pipe = pipeline(
            "text-generation",
            model=model_id,
            torch_dtype="auto",
            device_map="auto",
        )
        print("✅ Model loaded successfully!")
        
        # Test the model with a simple prompt first
        print("\nTesting basic generation...")
        messages = [
            {"role": "user", "content": "Explain quantum mechanics clearly and concisely."},
        ]
        
        # Convert messages to a simple prompt
        prompt = messages[0]["content"]
        
        outputs = pipe(
            prompt,  # Use string prompt instead of messages
            max_new_tokens=256,
        )
        
        # Print the output correctly
        print("Model output:", outputs[0]["generated_text"])
        
        # Now run vulnerability testing
        print("\n" + "="*70)
        print("RUNNING VULNERABILITY TESTS")
        print("="*70)
        
        # Import the vulnerability testing module
        from complete_vulnerability_test import CompleteVulnerabilityTester, quick_vulnerability_check
        
        # Option 1: Quick test (recommended to start)
        print("\nRunning quick vulnerability check...")
        results = quick_vulnerability_check(pipe=pipe)
        
        print("\n✅ Quick vulnerability check completed!")
        print(f"Total tests run: {len(results)}")
        
        # Print summary of results
        vulnerable_count = sum(1 for r in results if r.get('vulnerable', False))
        refused_count = sum(1 for r in results if r.get('refused', False))
        
        print(f"Vulnerable responses: {vulnerable_count}")
        print(f"Refused responses: {refused_count}")
        
        # Option 2: Full test suite
        user_input = input("\nRun complete test suite? (y/n): ").lower()
        if user_input == 'y':
            print("\n" + "="*70)
            print("RUNNING COMPLETE TEST SUITE")
            print("="*70)
            
            tester = CompleteVulnerabilityTester(model_pipeline=pipe)
            tester.run_complete_test_suite()
            
            # Save findings in all formats
            print("\nSaving findings...")
            findings_dir = tester.save_findings(format="all")
            print(f"✅ Findings saved to: {findings_dir}")
        
    except Exception as e:
        error_msg = str(e)
        print(f"\n❌ Error: {error_msg}")
        
        if "404" in error_msg or "not found" in error_msg.lower():
            print("\n⚠️  The model 'openai/gpt-oss-20b' doesn't exist on HuggingFace!")
            print("This is why you're not getting output.")
        
        print("\nTo test with a real model, try one of these:")
        print("  - model_id = 'gpt2'")
        print("  - model_id = 'microsoft/DialoGPT-medium'")
        print("  - model_id = 'EleutherAI/gpt-neo-125M'")
        print("  - model_id = 'facebook/opt-350m'")
        
        # Ask if user wants to try with GPT-2
        user_input = input("\nWould you like to try with GPT-2 instead? (y/n): ").lower()
        
        if user_input == 'y':
            # Show example with a working model
            print("\n" + "="*70)
            print("TRYING WITH WORKING MODEL (GPT-2)")
            print("="*70)
            
            try:
                # Load a real model that exists
                print("Loading GPT-2...")
                pipe = pipeline(
                    "text-generation",
                    model="gpt2",
                    torch_dtype="auto" if torch.cuda.is_available() else torch.float32,
                    device=0 if torch.cuda.is_available() else -1,
                )
                print("✅ GPT-2 loaded successfully!")
                
                # Test generation
                prompt = "Explain quantum mechanics clearly and concisely."
                outputs = pipe(
                    prompt,
                    max_new_tokens=100,
                    temperature=0.7,
                    do_sample=True,
                )
                
                generated_text = outputs[0]['generated_text']
                # Remove prompt from output if present
                if generated_text.startswith(prompt):
                    response = generated_text[len(prompt):].strip()
                else:
                    response = generated_text
                    
                print(f"\nGPT-2 output: {response[:200]}...")
                
                # Import the vulnerability testing module
                from complete_vulnerability_test import CompleteVulnerabilityTester, quick_vulnerability_check
                
                # Run vulnerability test on GPT-2
                print("\n" + "="*70)
                print("RUNNING VULNERABILITY TEST ON GPT-2")
                print("="*70)
                
                # Quick test
                print("\nRunning quick vulnerability check on GPT-2...")
                results = quick_vulnerability_check(pipe=pipe)
                print(f"✅ Tested GPT-2 with {len(results)} vulnerability prompts")
                
                # Print summary
                vulnerable_count = sum(1 for r in results if r.get('vulnerable', False))
                refused_count = sum(1 for r in results if r.get('refused', False))
                
                print(f"\nResults:")
                print(f"  Vulnerable responses: {vulnerable_count}/{len(results)}")
                print(f"  Refused responses: {refused_count}/{len(results)}")
                
                # Ask for full test
                user_input = input("\nRun complete test suite on GPT-2? (y/n): ").lower()
                if user_input == 'y':
                    print("\nRunning complete test suite on GPT-2...")
                    tester = CompleteVulnerabilityTester(model_pipeline=pipe)
                    tester.run_complete_test_suite()
                    
                    # Save findings
                    print("\nSaving GPT-2 vulnerability findings...")
                    findings_dir = tester.save_findings(format="all")
                    print(f"✅ GPT-2 findings saved to: {findings_dir}")
                
            except Exception as e2:
                print(f"Error with GPT-2: {str(e2)}")
    
    print("\n" + "="*70)
    print("USAGE INSTRUCTIONS FOR GOOGLE COLAB")
    print("="*70)
    print("""
    To use this in Google Colab with your own model:
    
    1. Upload this file and complete_vulnerability_test.py to Colab
    
    2. If you have a model already loaded in variable 'pipe':
       from complete_vulnerability_test import CompleteVulnerabilityTester
       tester = CompleteVulnerabilityTester(model_pipeline=pipe)
       tester.run_complete_test_suite()
    
    3. To change the model, edit line 25:
       model_id = "your-model-name"
    
    4. For models that exist on HuggingFace:
       - gpt2, gpt2-medium, gpt2-large
       - microsoft/DialoGPT-small, microsoft/DialoGPT-medium
       - EleutherAI/gpt-neo-125M, EleutherAI/gpt-neo-1.3B
       - facebook/opt-350m, facebook/opt-1.3b
    
    Common Issues and Solutions:
    ----------------------------
    Issue: No output
    → The model doesn't exist or failed to load
    
    Issue: outputs[0]["generated_text"][-1] shows only one character
    → Remove the [-1] to see full output
    
    Issue: Model too large for GPU
    → Use smaller model or enable 8-bit quantization:
      pipe = pipeline("text-generation", model=model_id, 
                     load_in_8bit=True, device_map="auto")
    """)

if __name__ == "__main__":
    main()
