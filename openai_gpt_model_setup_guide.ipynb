{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Guide: Load OpenAI GPT-OSS-20B Model and Run Attacks\n",
    "\n",
    "This notebook will:\n",
    "1. Load the model properly\n",
    "2. Test that it's working\n",
    "3. Run the attack scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: GPU NOT ENABLED!\")\n",
    "    print(\"Please go to: Runtime → Change runtime type → GPU → Save\")\n",
    "    print(\"Then run this cell again\")\n",
    "else:\n",
    "    print(f\"GPU Enabled: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content')\n",
    "\n",
    "# Clone repository\n",
    "!rm -rf openai-\n",
    "!git clone https://github.com/Unknown1502/openai-.git openai-\n",
    "%cd openai-\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch\n",
    "!pip install -q transformers>=4.45.0\n",
    "!pip install -q accelerate>=0.25.0\n",
    "!pip install -q bitsandbytes>=0.41.3\n",
    "!pip install -q sentencepiece protobuf\n",
    "!pip install -q aiohttp nest-asyncio tqdm numpy pandas\n",
    "\n",
    "# For MXFP4 support\n",
    "!pip install -q triton>=3.4.0\n",
    "!pip install -q kernels\n",
    "\n",
    "print(\"All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fix the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply necessary fixes\n",
    "!sed -i 's/ENHANCED_COMPETITION_ATTACKS = ENHANCED_COMPETITION_ATTACK_VECTORS/# Already defined above/' src/competition/enhanced_attack_vectors.py\n",
    "print(\"Code fixes applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load the Model (Most Important Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set environment for better memory management\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "model_name = \"openai/gpt-oss-20b\"\n",
    "\n",
    "print(\"Loading model (this will take 5-10 minutes)...\")\n",
    "\n",
    "# Load tokenizer first\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Tokenizer loaded\")\n",
    "\n",
    "# Try to load model with MXFP4 dequantization\n",
    "print(\"\\nLoading model with dequantization...\")\n",
    "print(\"This will download ~14GB and may take several minutes...\")\n",
    "\n",
    "try:\n",
    "    # Force dequantization for MXFP4\n",
    "    os.environ['MXFP4_DEQUANTIZE'] = '1'\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        offload_folder=\"offload\",  # Use CPU offloading if needed\n",
    "        offload_state_dict=True\n",
    "    )\n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "    print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B parameters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading model: {str(e)[:500]}\")\n",
    "    print(\"\\nTrying alternative loading method...\")\n",
    "    \n",
    "    # Alternative: Load without quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Model loaded with alternative method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the model is working\n",
    "print(\"Testing model...\")\n",
    "\n",
    "test_prompt = \"Hello, I am an AI assistant. How can I help you today?\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nPrompt: {test_prompt}\")\n",
    "print(f\"Response: {response}\")\n",
    "print(\"\\nModel is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Model to Globals (For Competition Scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer globally so competition scripts can use them\n",
    "import sys\n",
    "\n",
    "# Create a module to hold the model\n",
    "class ModelHolder:\n",
    "    model = model\n",
    "    tokenizer = tokenizer\n",
    "\n",
    "sys.modules['model_holder'] = ModelHolder\n",
    "\n",
    "print(\"Model saved to global scope\")\n",
    "print(\"The competition scripts can now access the loaded model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Run Competition (Quick Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a quick test with the enhanced competition runner\n",
    "!python run_enhanced_competition.py --mode quick --strategy standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run Full Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the standard competition\n",
    "!python run_competition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Run with Custom Model Loading\n",
    "\n",
    "If the above doesn't work, use this custom runner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct attack test with loaded model\n",
    "from src.competition.attack_vectors import COMPETITION_ATTACK_VECTORS\n",
    "\n",
    "print(\"Running direct attack test...\")\n",
    "\n",
    "# Test first attack\n",
    "attack = COMPETITION_ATTACK_VECTORS[0]\n",
    "print(f\"\\nTesting: {attack.name}\")\n",
    "print(f\"Category: {attack.category}\")\n",
    "\n",
    "# Run the attack\n",
    "inputs = tokenizer(attack.prompt_template, return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nModel Response: {response}\")\n",
    "\n",
    "# Check for vulnerability\n",
    "if any(keyword in response.lower() for keyword in ['yes', 'admit', 'true', 'correct']):\n",
    "    print(\"\\nPotential vulnerability detected!\")\n",
    "else:\n",
    "    print(\"\\nNo vulnerability detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "If you encounter issues:\n",
    "\n",
    "1. **CUDA Out of Memory**: Restart runtime and try again\n",
    "2. **Model won't load**: Ensure you have GPU enabled (A100 preferred)\n",
    "3. **Import errors**: Re-run the dependency installation cell\n",
    "4. **No vulnerabilities detected**: The model might be too safe - try different prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
