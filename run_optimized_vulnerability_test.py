#!/usr/bin/env python3
"""
Optimized vulnerability test with memory management for large models
Handles CUDA OOM errors and provides alternative solutions
"""

from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch
import sys
import os
import gc

# Add current directory to path to import the test module
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def clear_gpu_memory():
    """Clear GPU memory to prevent OOM errors"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
        print(f"GPU memory cleared. Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

def load_model_with_optimization(model_id, use_8bit=True, use_4bit=False, use_cpu_offload=False):
    """
    Load model with memory optimizations
    """
    clear_gpu_memory()
    
    print(f"\nAttempting to load: {model_id}")
    print(f"Optimization: {'4-bit' if use_4bit else '8-bit' if use_8bit else 'CPU offload' if use_cpu_offload else 'None'}")
    
    try:
        # Check if model exists and is accessible
        if "gpt-oss" in model_id:
            print("⚠️  Warning: 'gpt-oss' models may not exist on HuggingFace")
            print("   Attempting to load anyway...")
        
        # Prepare loading arguments
        load_kwargs = {
            "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
            "low_cpu_mem_usage": True,
        }
        
        # Use CPU offloading for very large models
        if use_cpu_offload:
            load_kwargs["device_map"] = {
            try:
                from transformers import BitsAndBytesConfig
                load_kwargs["quantization_config"] = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                print("Using 4-bit quantization")
            except ImportError:
                print("BitsAndBytes not available, falling back to 8-bit")
                use_8bit = True
                use_4bit = False
        
        if use_8bit and not use_4bit and torch.cuda.is_available():
            load_kwargs["load_in_8bit"] = True
            print("Using 8-bit quantization")
        
        # Try to load with pipeline
        pipe = pipeline(
            "text-generation",
            model=model_id,
            **load_kwargs
        )
        
