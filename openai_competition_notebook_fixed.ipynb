{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI GPT-OSS-20B Competition Notebook (Fixed)\n",
    "\n",
    "This notebook runs the red-teaming competition against the OpenAI gpt-oss-20b model with all necessary fixes applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q sentencepiece protobuf\n",
    "!pip install -q nest-asyncio aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository if not already present\n",
    "import os\n",
    "if not os.path.exists('/content/openai-'):\n",
    "    !git clone https://github.com/your-repo/openai-.git /content/openai-\n",
    "    %cd /content/openai-\n",
    "else:\n",
    "    %cd /content/openai-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Critical Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the device mismatch fix to hf_local.py\n",
    "fix_content = '''# Fix for device mismatch in hf_local.py\n",
    "import fileinput\n",
    "import sys\n",
    "\n",
    "# Read the file and apply the fix\n",
    "with open('src/backends/hf_local.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Check if fix is already applied\n",
    "if 'Move inputs to the same device as the model' not in content:\n",
    "    # Find the location to insert the fix\n",
    "    search_text = '        # Tokenize (remain on CPU; Transformers/Accelerate will dispatch as needed)\\\\n        inputs = self._tokenizer(text, return_tensors=\"pt\")'\n",
    "    \n",
    "    replacement_text = \"\"\"        # Tokenize (remain on CPU; Transformers/Accelerate will dispatch as needed)\n",
    "        inputs = self._tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Move inputs to the same device as the model\n",
    "        if hasattr(self._model, 'device'):\n",
    "            device = self._model.device\n",
    "        elif hasattr(self._model, 'module') and hasattr(self._model.module, 'device'):\n",
    "            device = self._model.module.device\n",
    "        else:\n",
    "            # Try to get device from model parameters\n",
    "            try:\n",
    "                device = next(self._model.parameters()).device\n",
    "            except:\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Move all input tensors to the correct device\n",
    "        inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}\"\"\"\n",
    "    \n",
    "    content = content.replace(search_text, replacement_text)\n",
    "    \n",
    "    # Write the fixed content back\n",
    "    with open('src/backends/hf_local.py', 'w') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    print(\"‚úÖ Applied device mismatch fix to hf_local.py\")\n",
    "else:\n",
    "    print(\"‚úÖ Device mismatch fix already applied\")\n",
    "'''\n",
    "\n",
    "exec(fix_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add current directory to Python path\n",
    "if '/content/openai-' not in sys.path:\n",
    "    sys.path.insert(0, '/content/openai-')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['TORCH_HOME'] = '/content/openai-/.cache/torch'\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Create cache directory\n",
    "os.makedirs('/content/openai-/.cache/torch', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify GPU and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"   Free Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU available\")\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Basic Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the basic competition\n",
    "!cd /content/openai- && python run_competition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Enhanced Competition (Quick Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run enhanced competition in quick mode to avoid memory issues\n",
    "!cd /content/openai- && python run_enhanced_competition.py --strategy standard --mode quick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Alternative: Run with Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach using Python directly\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def run_competition_async():\n",
    "    \"\"\"Run competition asynchronously\"\"\"\n",
    "    try:\n",
    "        # Import after environment setup\n",
    "        from src.competition.enhanced_attack_vectors import (\n",
    "            ENHANCED_COMPETITION_ATTACK_VECTORS,\n",
    "            get_attack_scenarios_by_severity,\n",
    "            Severity\n",
    "        )\n",
    "        from src.core.client_factory import ClientFactory\n",
    "        from src.core.vulnerability_scanner import VulnerabilityScanner\n",
    "        from src.config import load_config\n",
    "        from src.utils.memory_manager import prepare_for_model_loading\n",
    "        \n",
    "        print(\"üöÄ Starting competition...\")\n",
    "        \n",
    "        # Check memory\n",
    "        can_load, memory_message = prepare_for_model_loading(20.0)\n",
    "        print(f\"Memory status: {memory_message}\")\n",
    "        \n",
    "        # Load configuration\n",
    "        config = load_config()\n",
    "        \n",
    "        # Create client\n",
    "        print(\"Loading model...\")\n",
    "        client = ClientFactory.create_client(config)\n",
    "        scanner = VulnerabilityScanner(client)\n",
    "        \n",
    "        # Get a few test scenarios\n",
    "        scenarios = get_attack_scenarios_by_severity(Severity.HIGH)[:3]\n",
    "        \n",
    "        print(f\"\\nTesting {len(scenarios)} scenarios...\")\n",
    "        \n",
    "        findings = []\n",
    "        async with client:\n",
    "            for i, scenario in enumerate(scenarios, 1):\n",
    "                print(f\"\\n[{i}/{len(scenarios)}] Testing: {scenario.name}\")\n",
    "                try:\n",
    "                    result = await scanner.test_vulnerability(\n",
    "                        prompt=scenario.prompt_template,\n",
    "                        category=scenario.category,\n",
    "                        expected_behavior=scenario.expected_vulnerability\n",
    "                    )\n",
    "                    \n",
    "                    if result.is_vulnerable:\n",
    "                        print(f\"  ‚úÖ Vulnerability found!\")\n",
    "                        findings.append(result)\n",
    "                    else:\n",
    "                        print(f\"  ‚ùå No vulnerability detected\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå Error: {e}\")\n",
    "        \n",
    "        print(f\"\\nüéâ Competition complete!\")\n",
    "        print(f\"Found {len(findings)} vulnerabilities\")\n",
    "        \n",
    "        return findings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Competition failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# Run the competition\n",
    "findings = await run_competition_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all result files\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Check for result files\n",
    "result_files = [\n",
    "    'competition_summary.json',\n",
    "    'enhanced_competition_report_standard_quick.json'\n",
    "]\n",
    "\n",
    "for file in result_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"\\nüìÑ Found: {file}\")\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if 'summary' in data:\n",
    "                print(f\"   Vulnerabilities found: {data['summary'].get('vulnerabilities_found', 0)}\")\n",
    "                print(f\"   Success rate: {data['summary'].get('success_rate', 'N/A')}\")\n",
    "\n",
    "# List all finding files\n",
    "finding_files = [f for f in os.listdir('.') if f.startswith('finding_') and f.endswith('.json')]\n",
    "if finding_files:\n",
    "    print(f\"\\nüìÅ Individual findings: {len(finding_files)} files\")\n",
    "    for f in finding_files[:5]:  # Show first 5\n",
    "        print(f\"   - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you encounter issues, run this cell for diagnostics\n",
    "\n",
    "print(\"üîç Diagnostics:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "# Check PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "except Exception as e:\n",
    "    print(f\"PyTorch error: {e}\")\n",
    "\n",
    "# Check transformers\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"Transformers: {transformers.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Transformers error: {e}\")\n",
    "\n",
    "# Check file structure\n",
    "print(\"\\nFile structure:\")\n",
    "for root, dirs, files in os.walk('src'):\n",
    "    level = root.replace('src', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:3]:  # Show first 3 files per directory\n",
    "        if file.endswith('.py'):\n",
    "            print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete solution for running the OpenAI GPT-OSS-20B red-teaming competition with the following fixes:\n",
    "\n",
    "1. **Device Mismatch Fix**: Ensures input tensors are moved to the same device as the model\n",
    "2. **Memory Management**: Uses quick mode to avoid OOM errors\n",
    "3. **Error Handling**: Properly handles cases where no vulnerabilities are found\n",
    "4. **Environment Setup**: Configures all necessary paths and variables\n",
    "\n",
    "The competition tests the model against various attack scenarios to identify potential vulnerabilities in areas like:\n",
    "- Deceptive alignment\n",
    "- Context manipulation\n",
    "- Tool misuse\n",
    "- Reward hacking\n",
    "- Sandbagging\n",
    "\n",
    "Results are saved in JSON format for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
